{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/anirudhg15/tps-jan-22-extensive-time-series-tutorial?scriptVersionId=135566331\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# TIME SERIES tutorial using JAN 2022 TPS data ðŸ“ˆðŸ“Š\n\nHi to the community!\n\nIn this public notebook, I build a tutorial on time series using both classic and ML based methods. Some of the concepts we will be discussing includes:\n- Moving Average\n- Weighted Moving Average\n- Exponential Smoothing\n- Holt model\n- Holt-Winter model\n- Stationarity\n- Feature extraction for time-series\n- Boosting models for time-series\n\nI also use the following libraries to the create the models:\n- Statsmodels\n- Scikit-learn\n- XGBoost\n\n**Note: This is a tutorial on time series using the TPS data. Therefore, I do not focus on the full data or a submission. I leverage the TPS data in the simplest way possible to teach/practise a tutorial on time-series**","metadata":{}},{"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:maroon; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>If you find this notebook useful, do give an upvote before forking it </center></h2>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport statsmodels.api as sm\nimport statsmodels.tsa.api as smt\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom xgboost import XGBRegressor","metadata":{"execution":{"iopub.status.busy":"2022-01-02T06:12:27.866162Z","iopub.execute_input":"2022-01-02T06:12:27.867138Z","iopub.status.idle":"2022-01-02T06:12:27.871419Z","shell.execute_reply.started":"2022-01-02T06:12:27.867089Z","shell.execute_reply":"2022-01-02T06:12:27.870736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tree ../input/","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:50:52.68868Z","iopub.execute_input":"2022-01-02T05:50:52.68909Z","iopub.status.idle":"2022-01-02T05:50:53.487058Z","shell.execute_reply.started":"2022-01-02T05:50:52.689059Z","shell.execute_reply":"2022-01-02T05:50:53.486063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We only use the training data from TPS for this tutorial","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv', \n           parse_dates=['date'], \n           index_col = ['date'])\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:50:54.001228Z","iopub.execute_input":"2022-01-02T05:50:54.001582Z","iopub.status.idle":"2022-01-02T05:50:54.044365Z","shell.execute_reply.started":"2022-01-02T05:50:54.001534Z","shell.execute_reply":"2022-01-02T05:50:54.043382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Country info in the data-\\n', df['country'].value_counts(), ' \\n')\nprint('Store info in the data-\\n', df['store'].value_counts(), ' \\n')\nprint('Product info in the data-\\n', df['product'].value_counts(), ' \\n')","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:50:54.500856Z","iopub.execute_input":"2022-01-02T05:50:54.501141Z","iopub.status.idle":"2022-01-02T05:50:54.515065Z","shell.execute_reply.started":"2022-01-02T05:50:54.501115Z","shell.execute_reply":"2022-01-02T05:50:54.514089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the data is equally distributed across the various categories in the columns, I am filtering the data for the following values, so that the data is simple for the tutorial:\n- Country : Sweden\n- Store: KaggleMart\n- Product: Kaggle Mug\n- Index: Only 1 year of data i.e the year of 2015","metadata":{}},{"cell_type":"code","source":"df = df.loc[(df['country']=='Sweden') & (df['store']=='KaggleMart') & (df['product']=='Kaggle Mug')]\ndf = df.drop(['row_id', 'country', 'store', 'product'], axis=1)\ndf = df[:'2015-12-31']\ndf","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:50:55.639735Z","iopub.execute_input":"2022-01-02T05:50:55.640581Z","iopub.status.idle":"2022-01-02T05:50:55.662514Z","shell.execute_reply.started":"2022-01-02T05:50:55.640545Z","shell.execute_reply":"2022-01-02T05:50:55.661908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplt.plot(df['num_sold'])","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:50:56.113851Z","iopub.execute_input":"2022-01-02T05:50:56.114846Z","iopub.status.idle":"2022-01-02T05:50:56.351343Z","shell.execute_reply.started":"2022-01-02T05:50:56.114786Z","shell.execute_reply":"2022-01-02T05:50:56.350545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MOVING AVERAGE\n\nMoving average is nothing but the average of last n observations in a rolling manner i.e an average of every 30 data points. In our case, here I take the average of last 30 observations","metadata":{}},{"cell_type":"code","source":"def moving_avg(col, last_n):\n    '''Calculate of the average of last n observations'''\n    \n    return np.mean(col[-last_n:])\n\nmoving_avg(df, 30)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:50:57.139626Z","iopub.execute_input":"2022-01-02T05:50:57.14009Z","iopub.status.idle":"2022-01-02T05:50:57.148713Z","shell.execute_reply.started":"2022-01-02T05:50:57.140042Z","shell.execute_reply":"2022-01-02T05:50:57.148017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"moving_avg = df.rolling(window=30).mean()\n#Pandas way of calculating the same for every 30 days automatically\n\nplt.figure(figsize=(15, 5))\nplt.title(\"Moving average\\n window size = {}\".format(30))\nplt.plot(moving_avg, label=\"Rolling Average\")\nplt.plot(df[30:], label='actual values')\nplt.legend(loc='best')","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:50:57.697738Z","iopub.execute_input":"2022-01-02T05:50:57.698244Z","iopub.status.idle":"2022-01-02T05:50:57.911223Z","shell.execute_reply.started":"2022-01-02T05:50:57.698211Z","shell.execute_reply":"2022-01-02T05:50:57.910394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WEIGHTED MOVING AVERAGE\nA slightly more complex approach is possible, which is weighted moving average - the total sums up to 1, where the recent K data points are given more weight","metadata":{}},{"cell_type":"code","source":"w_moving_avg = df.rolling(window=30, win_type='cosine').mean()\n# The weight comes from the win_type parameter. Check out the official docs for all available parameter options\n\nplt.figure(figsize=(15, 5))\nplt.title(\"Moving average\\n window size = {}\".format(30))\nplt.plot(w_moving_avg, label=\"Rolling Average\")\nplt.plot(df[30:], label='actual values')\nplt.legend(loc='best')","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:50:58.822911Z","iopub.execute_input":"2022-01-02T05:50:58.823602Z","iopub.status.idle":"2022-01-02T05:50:59.088034Z","shell.execute_reply.started":"2022-01-02T05:50:58.823561Z","shell.execute_reply":"2022-01-02T05:50:59.087235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXPONENTIAL SMOOTHING\n\nInstead of weighting the last K values, we weight all the data points. Here as we move back in time, the weights are exponentially decreased. It can be expressed by:\n\n$$\\hat{y}_{t} = \\alpha \\cdot y_t + (1-\\alpha) \\cdot \\hat y_{t-1}$$\n\nHere, aplha is the weight of smoothing factor. It defines how much the last observation affects the current. The smaller the alpha, the more influence the previous data point has --> the statsmodels parameter for this is 'smoothing_level'","metadata":{}},{"cell_type":"code","source":"ses = smt.SimpleExpSmoothing(df, \n                            initialization_method='heuristic').fit(smoothing_level=0.3, \n                                                                    optimized=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:50:59.946144Z","iopub.execute_input":"2022-01-02T05:50:59.946831Z","iopub.status.idle":"2022-01-02T05:50:59.95529Z","shell.execute_reply.started":"2022-01-02T05:50:59.946792Z","shell.execute_reply":"2022-01-02T05:50:59.954449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.plot(df, marker=\"o\", color=\"orange\")\nplt.plot(ses.fittedvalues, marker=\"o\", color=\"blue\")\nplt.legend(loc='best')","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:51:00.500288Z","iopub.execute_input":"2022-01-02T05:51:00.501147Z","iopub.status.idle":"2022-01-02T05:51:00.75897Z","shell.execute_reply.started":"2022-01-02T05:51:00.501106Z","shell.execute_reply":"2022-01-02T05:51:00.758189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Holt model\n\nPreviously, we only took the smoothing level into account. Now we also take the smoothing trend into account. So the formula is updated as: \n\n$$\\ell_x = \\alpha y_x + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$$\n\n$$b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$$\n\n$$\\hat{y}_{x+1} = \\ell_x + b_x$$\n\nHere, beta is the trend factor. The final formula as hown depends on both trend & level","metadata":{}},{"cell_type":"code","source":"holt = smt.Holt(endog=df,\n            initialization_method='heuristic').fit(smoothing_level=0.3, \n                                                   smoothing_trend =0.9, \n                                                   optimized=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:51:01.725231Z","iopub.execute_input":"2022-01-02T05:51:01.72553Z","iopub.status.idle":"2022-01-02T05:51:01.73712Z","shell.execute_reply.started":"2022-01-02T05:51:01.725502Z","shell.execute_reply":"2022-01-02T05:51:01.736364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,7))\nplt.plot(df, marker=\"o\", color=\"orange\")\nplt.plot(holt.fittedvalues, marker=\"o\", color=\"blue\")","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:51:02.297252Z","iopub.execute_input":"2022-01-02T05:51:02.29753Z","iopub.status.idle":"2022-01-02T05:51:02.562112Z","shell.execute_reply.started":"2022-01-02T05:51:02.297503Z","shell.execute_reply":"2022-01-02T05:51:02.561284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Holt-Winter model\n\nAlong with trend & level, we add the seasonal component here. So the formula becomes:\n\n$$\\ell_x = \\alpha(y_x - s_{x-L}) + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$$\n\n$$b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$$\n\n$$s_x = \\gamma(y_x - \\ell_x) + (1-\\gamma)s_{x-L}$$\n\n$$\\hat{y}_{x+m} = \\ell_x + mb_x + s_{x-L+1+(m-1)modL}$$\n\nHere, gamma is the seasonal component","metadata":{}},{"cell_type":"markdown","source":"**I am leaving the implementation of Holt-Winters as exercise for the reader.**\n\n**Even I want to implement holt-winters from scratch in plain python. So expect a separate notebook on it in the near future**","metadata":{}},{"cell_type":"markdown","source":"# Stationarity\n\nStationarity mean that the time-series properties like mean, variance etc stay the same throughout the series. It is easier to make predictions on a time-series if we know that the future properties of the series will same as the current data points\n\nWe can check for stationarity of a time series using the *dickey-fuller test*. The resuling p value should be less than 0.05 to reject the null hypothesis i.e to reject that the series is non-stationary","metadata":{}},{"cell_type":"code","source":"y = pd.Series(df.num_sold)\np_value = sm.tsa.stattools.adfuller(y)[1]\nprint(p_value)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:51:04.447377Z","iopub.execute_input":"2022-01-02T05:51:04.447632Z","iopub.status.idle":"2022-01-02T05:51:04.466538Z","shell.execute_reply.started":"2022-01-02T05:51:04.447606Z","shell.execute_reply":"2022-01-02T05:51:04.465553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, the p-value id 0.9, which is greater than 0.05. So the null hypothesis told true & the series is non-stationary","metadata":{}},{"cell_type":"markdown","source":"One way to deal with stationarity is to take the log of the series & then take the first difference i.e the difference of the series with itself\n\n(Please breakdown the code in the following cell to see the differencing process in action)","metadata":{}},{"cell_type":"code","source":"y = pd.Series(np.log(df[\"num_sold\"])).diff().dropna()\np_value = sm.tsa.stattools.adfuller(y)[1]\nprint(p_value)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:51:06.090594Z","iopub.execute_input":"2022-01-02T05:51:06.09121Z","iopub.status.idle":"2022-01-02T05:51:06.110015Z","shell.execute_reply.started":"2022-01-02T05:51:06.091165Z","shell.execute_reply":"2022-01-02T05:51:06.109052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, the p-value is 0.002 i.e 2%. Therefore we can reject the null hypothesis & say that the series is stationary","metadata":{}},{"cell_type":"markdown","source":"# Feature extraction for time-series\n\nAny ML model needs features, but all we have here is a 1-dimensional feature (which we simplified wantedly). So, based on this 1-d feature, how to get more features? Thats where a few feature extraction techniques come in handy\n\nOne such technique is to calculate the lag iteratively for a given range","metadata":{}},{"cell_type":"code","source":"data = pd.DataFrame(df.num_sold.copy())\ndata.columns = [\"y\"]\nfor i in range(6, 20):\n    data[\"lag_{}\".format(i)] = data.y.shift(i)\n\ndata.tail(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:51:08.146888Z","iopub.execute_input":"2022-01-02T05:51:08.147307Z","iopub.status.idle":"2022-01-02T05:51:08.191907Z","shell.execute_reply.started":"2022-01-02T05:51:08.147254Z","shell.execute_reply":"2022-01-02T05:51:08.19102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another technique is to extract the date properties like weekday, weekend, hour etc as separate features","metadata":{}},{"cell_type":"code","source":"data.index = pd.to_datetime(data.index)\ndata[\"weekday\"] = data.index.weekday\ndata[\"is_weekend\"] = data.weekday.isin([5, 6]) * 1\ndata.tail(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:51:09.420067Z","iopub.execute_input":"2022-01-02T05:51:09.420383Z","iopub.status.idle":"2022-01-02T05:51:09.447866Z","shell.execute_reply.started":"2022-01-02T05:51:09.420347Z","shell.execute_reply":"2022-01-02T05:51:09.447035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Boosting models for time-series\n\nTo create and validate an ML model, we need train & test set. \n\nHowever, in time-series we cannot use the scikit's learning train_test_split() function as it shuffles the data & thereby messing with the time factor. So to keep the series intact, I have reserved the decemeber month's data as test set & the rest as train set","metadata":{}},{"cell_type":"code","source":"train = data[:'2015-11-30'].copy()\ntest = data['2015-12-01':].copy()","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:57:24.431951Z","iopub.execute_input":"2022-01-02T05:57:24.432297Z","iopub.status.idle":"2022-01-02T05:57:24.441828Z","shell.execute_reply.started":"2022-01-02T05:57:24.432262Z","shell.execute_reply":"2022-01-02T05:57:24.440882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.tail(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:57:26.166593Z","iopub.execute_input":"2022-01-02T05:57:26.167106Z","iopub.status.idle":"2022-01-02T05:57:26.189301Z","shell.execute_reply.started":"2022-01-02T05:57:26.167056Z","shell.execute_reply":"2022-01-02T05:57:26.188419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.tail(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:57:35.746423Z","iopub.execute_input":"2022-01-02T05:57:35.746927Z","iopub.status.idle":"2022-01-02T05:57:35.769215Z","shell.execute_reply.started":"2022-01-02T05:57:35.746877Z","shell.execute_reply":"2022-01-02T05:57:35.768216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train.dropna().y\nX_train = train.dropna().drop([\"y\"], axis=1)\n\ny_test = test.dropna().y\nX_test = test.dropna().drop([\"y\"], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T06:02:34.170824Z","iopub.execute_input":"2022-01-02T06:02:34.171554Z","iopub.status.idle":"2022-01-02T06:02:34.187531Z","shell.execute_reply.started":"2022-01-02T06:02:34.171512Z","shell.execute_reply":"2022-01-02T06:02:34.18661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_results(model, X_train = X_train, X_test = X_test):\n    \n    '''Helper function to plot the actual & predicted values from the model'''\n    \n    prediction = model.predict(X_test)\n\n    plt.figure(figsize=(15, 7))\n    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n    error = metrics.mean_squared_error(prediction, y_test)\n    plt.title(\"Mean squared error {0:.2f}\".format(error))\n    plt.legend(loc='best')","metadata":{"execution":{"iopub.status.busy":"2022-01-02T06:16:42.690575Z","iopub.execute_input":"2022-01-02T06:16:42.690889Z","iopub.status.idle":"2022-01-02T06:16:42.696944Z","shell.execute_reply.started":"2022-01-02T06:16:42.690857Z","shell.execute_reply":"2022-01-02T06:16:42.696071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The input features of the data here is in different scales with lag, weekday & is_weekend. We need to scale the data before proceeding to create a boosting model","metadata":{}},{"cell_type":"code","source":"scaler = preprocessing.StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T06:10:25.070065Z","iopub.execute_input":"2022-01-02T06:10:25.070648Z","iopub.status.idle":"2022-01-02T06:10:25.081669Z","shell.execute_reply.started":"2022-01-02T06:10:25.070613Z","shell.execute_reply":"2022-01-02T06:10:25.080963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb = XGBRegressor(verbosity=0)\nxgb.fit(X_train_scaled, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T06:15:06.103069Z","iopub.execute_input":"2022-01-02T06:15:06.103386Z","iopub.status.idle":"2022-01-02T06:15:06.565199Z","shell.execute_reply.started":"2022-01-02T06:15:06.103353Z","shell.execute_reply":"2022-01-02T06:15:06.564338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_results(xgb,X_train=X_train_scaled,X_test=X_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T06:16:45.650558Z","iopub.execute_input":"2022-01-02T06:16:45.651036Z","iopub.status.idle":"2022-01-02T06:16:45.910034Z","shell.execute_reply.started":"2022-01-02T06:16:45.650988Z","shell.execute_reply":"2022-01-02T06:16:45.909051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a huge error in predicting the last 5 days of the month. Others have been predicted decently well","metadata":{}},{"cell_type":"markdown","source":"### In Part 2 of the notebook, I will discuss how to optimize the prediction of this XGBoost model & discuss some advance concepts including:\n## - Time-series cross validation\n## - ARIMA family of models\n## - Reading ACF & PACF plots\n## - LSTM based time-series\n## - Prophet library for time series","metadata":{}},{"cell_type":"markdown","source":"# Share your thoughts & feedback in the comments\n##### Stay safe & enjoy a happy 2022 everyone. Cheers!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}